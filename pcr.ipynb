{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, make_scorer, balanced_accuracy_score\n",
    ")\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/ArvinCorotana/ML/main/TrainDataset2023.xls'\n",
    "dataset = pd.read_excel(url, sheet_name='Sheet1')\n",
    "dataset.replace(999, np.nan, inplace=True)\n",
    "dataset = dataset.dropna(subset=['pCR (outcome)', 'RelapseFreeSurvival (outcome)'], how='any')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def data_split(dataset):\n",
    "    dataset.drop('ID',axis=1,inplace=True)\n",
    "    X = dataset.iloc[:, 2:]\n",
    "    y = dataset.iloc[:, :2]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "    data = pd.concat((y_test,X_test),axis=1)\n",
    "    data = data.dropna(how='any')\n",
    "    X_test = data.iloc[:, 2:]\n",
    "    y_test = data.iloc[:,:2]\n",
    "    X_train = X_train.reset_index(drop=True)\n",
    "    y_train = y_train.reset_index(drop=True)\n",
    "    X_test = X_test.reset_index(drop=True)\n",
    "    y_test = y_test.reset_index(drop=True)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "def imputation(X_train):\n",
    "    imputer = SimpleImputer(strategy='most_frequent')\n",
    "    imputer.fit(X_train)\n",
    "    X_train_imputed = imputer.transform(X_train)\n",
    "    X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train.columns, index=X_train.index)\n",
    "    with open('imputer.pkl', 'wb') as file:\n",
    "        pickle.dump(imputer, file)\n",
    "    return X_train_imputed\n",
    "\n",
    "def outlier(mri_features):\n",
    "\n",
    "    mri_features_copy = mri_features.copy()\n",
    "    Q1 = mri_features_copy.quantile(0.25)\n",
    "    Q3 = mri_features_copy.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_lim = Q1 - (1.5 * IQR)\n",
    "    upper_lim = Q3 + (1.5 * IQR)\n",
    "\n",
    "\n",
    "    for column in mri_features_copy:\n",
    "        outliers = ((mri_features_copy[column] < lower_lim[column]) |\n",
    "                    (mri_features_copy[column] > upper_lim[column]))\n",
    "\n",
    "\n",
    "        mri_features_copy.loc[outliers, column] = mri_features_copy.loc[outliers, column].apply(lambda x: lower_lim[column] if x < lower_lim[column] else upper_lim[column])\n",
    "\n",
    "    return mri_features_copy\n",
    "\n",
    "def target(y_train,y_test):\n",
    "    y_train = y_train.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "    y_train = y_train.squeeze()\n",
    "    y_test = y_test.drop('RelapseFreeSurvival (outcome)', axis=1)\n",
    "    y_test = y_test.squeeze()\n",
    "    return y_train,y_test\n",
    "\n",
    "def oversampling(X_train,y_train):\n",
    "    sm = SMOTE(sampling_strategy='minority', k_neighbors=5, random_state=42)\n",
    "    X_train, y_train = sm.fit_resample(X_train, y_train)\n",
    "    return X_train,y_train\n",
    "\n",
    "\n",
    "def normalization(X_train,X_test_imputed):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(X_train)\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "    X_test_scaled = scaler.transform(X_test_imputed)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test_imputed.columns)\n",
    "    with open('minmax.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "    return X_train,X_test_imputed\n",
    "\n",
    "\n",
    "def feature_selection(X_train_new,X_test_new):\n",
    "    correlation_matrix = X_train_new.corr()\n",
    "    correlation_matrix = abs(correlation_matrix)\n",
    "    mask = np.triu(np.ones(correlation_matrix.shape), k=1)\n",
    "    correlation_features = correlation_matrix[correlation_matrix > 0.85]\n",
    "    columns_to_drop = [column for column in correlation_features.columns if (correlation_features[column] > 0).sum() > 3]\n",
    "    X_train_new = X_train_new.drop(columns=columns_to_drop)\n",
    "    X_test_new = X_test_new.drop(columns=columns_to_drop)\n",
    "    columns = X_train.columns.tolist()\n",
    "    with open('features.pkl', 'wb') as file:\n",
    "        pickle.dump(columns, file)\n",
    "    return X_train_new,X_test_new\n",
    "\n",
    "\n",
    "def data_processing(dataset):\n",
    "    X_train, X_test, y_train, y_test = data_split(dataset)\n",
    "    X_train = imputation(X_train)\n",
    "    y_train,y_test = target(y_train,y_test)\n",
    "    X_train,y_train = oversampling(X_train,y_train)\n",
    "    X_train_clinical = X_train.iloc[:,:10]\n",
    "    X_test_clinical = X_test.iloc[:,:10]\n",
    "    X_train_mri = X_train.iloc[:,10:]\n",
    "    X_test_mri = X_test.iloc[:,10:]\n",
    "    X_train_mri = outlier(X_train_mri)\n",
    "    X_train_mri,X_test_mri = normalization(X_train_mri,X_test_mri)\n",
    "    X_train_mri,X_test_mri = feature_selection(X_train_mri,X_test_mri)\n",
    "    X_train = pd.concat([X_train_clinical, X_train_mri], axis=1)\n",
    "    X_test= pd.concat([X_test_clinical, X_test_mri], axis=1)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "\n",
    "def MLP(X_train, X_test, y_train, y_test):\n",
    "    print('mlp')\n",
    "    mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'invscaling', 'adaptive']\n",
    "    }\n",
    "    grid_search = GridSearchCV(mlp, param_grid, scoring='balanced_accuracy', cv=5)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_mlp = grid_search.best_estimator_\n",
    "    with open('best_mlp_model.pkl', 'wb') as file:\n",
    "        pickle.dump(grid_search, file)    \n",
    "    print(\"Best parameters found during GridSearchCV:\", grid_search.best_params_)\n",
    "    y_pred = best_mlp.predict(X_test)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f'Balanced Accuracy on test set: {balanced_accuracy:.4f}')\n",
    "    cv_scores = cross_val_score(best_mlp, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='balanced_accuracy')\n",
    "    print(\"Cross-Validation Accuracy Score for each fold:\", cv_scores)\n",
    "    print(f\"Mean CV Accuracy for the best MLP model: {cv_scores.mean():.2%}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def svc(X_train, X_test, y_train, y_test):\n",
    "    print('SVC')\n",
    "    svm = SVC()\n",
    "    svm.fit(X_train, y_train)\n",
    "    y_pred_default = svm.predict(X_test)\n",
    "    print(\"Default SVM Accuracy:\", svm.score(X_test, y_test))\n",
    "    param_grid = {'C': [0.01, 1, 10],\n",
    "                  'gamma': [0.01, 1, 'scale'],\n",
    "                  'kernel': ['linear', 'rbf']}\n",
    "    grid_search = GridSearchCV(svm, param_grid, cv=StratifiedKFold(n_splits=5), scoring='balanced_accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    final_svm = grid_search.best_estimator_\n",
    "    with open('best_svm_model.pkl', 'wb') as file:\n",
    "        pickle.dump(final_svm, file)\n",
    "    accuracy = final_svm.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy on test set: {accuracy: .2%}\")\n",
    "    y_pred = final_svm.predict(X_test)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f\"Balanced Accuracy for the best SVM model: {balanced_accuracy:.2%}\")\n",
    "    cv_scores = cross_val_score(final_svm, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='balanced_accuracy')\n",
    "    print(\"Cross-Validation Accuracy Score for each fold:\", cv_scores)\n",
    "    print(f\"Mean CV Accuracy for the best SVM model: {cv_scores.mean():.2%}\")\n",
    "    accuracys = accuracy_score(y_test, y_pred)\n",
    "    precisions = precision_score(y_test, y_pred)\n",
    "    recalls = recall_score(y_test, y_pred)\n",
    "    print(\"Accuracy:\", accuracys)\n",
    "    print(\"Precision:\", precisions)\n",
    "    print(\"Recall:\", recalls)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "def logistic_regression(X_train, X_test, y_train, y_test):\n",
    "    print('lr')\n",
    "    logreg = LogisticRegression(random_state=42)\n",
    "\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        'solver': ['liblinear', 'saga']\n",
    "    }\n",
    "\n",
    "    grid_searchlr = GridSearchCV(logreg, param_grid, scoring='balanced_accuracy', cv=5)\n",
    "    grid_searchlr.fit(X_train, y_train)\n",
    "\n",
    "    best_logreg = grid_searchlr.best_estimator_\n",
    "\n",
    "    print(\"Best parameters found during GridSearchCV:\", grid_searchlr.best_params_)\n",
    "    with open('best_lr_model.pkl', 'wb') as file:\n",
    "        pickle.dump(grid_searchlr, file)\n",
    "\n",
    "    cv_scores = cross_val_score(best_logreg, X_train, y_train, cv=StratifiedKFold(n_splits=5), scoring='balanced_accuracy')\n",
    "    print(\"Cross-Validation Accuracy Score for each fold:\", cv_scores)\n",
    "    print(f\"Mean CV Accuracy for the best SVM model: {cv_scores.mean():.2%}\")\n",
    "    y_pred = best_logreg.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy on test set: {accuracy:.4f}')\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f'Balanced Accuracy on test set: {balanced_accuracy:.4f}')\n",
    "\n",
    "def decision_Tree(X_train, X_test, y_train, y_test):\n",
    "    print('DT')\n",
    "    dt = DecisionTreeClassifier(random_state=42)\n",
    "    dt.fit(X_train, y_train)\n",
    "\n",
    "    accuracy = dt.score(X_test, y_test)\n",
    "    print(f\"Default Model Accuracy on test set: {accuracy: .2%}\")\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [None, 5, 10, 15, 20, 25, 30],\n",
    "        'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        'min_samples_leaf': [1, 2, 4, 8, 12],\n",
    "        'max_features': ['sqrt', 'log2', None],\n",
    "        'class_weight': [None, 'balanced']\n",
    "    }\n",
    "    cv = StratifiedKFold(n_splits=3)\n",
    "\n",
    "    grid_searchd = GridSearchCV(dt, param_grid, scoring='balanced_accuracy', cv=cv)\n",
    "    grid_searchd.fit(X_train, y_train)\n",
    "\n",
    "    final_dt = grid_searchd.best_estimator_\n",
    "    accuracy = final_dt.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy on test set: {accuracy: .2%}\")\n",
    "\n",
    "    with open('best_dt_model.pkl', 'wb') as file:\n",
    "        pickle.dump(grid_searchd, file)\n",
    "\n",
    "    y_pred = final_dt.predict(X_test)\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
    "    print(f'Balanced Accuracy: {balanced_accuracy:.4f}')\n",
    "\n",
    "def RandomForest(X_train, X_test, y_train, y_test):\n",
    "    print('Random Forest')\n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, y_train)\n",
    "    print(f'Model Accuracy = {rf.score(X_test, y_test): .2%}')\n",
    "    param_grid = {'n_estimators': [50, 100, 150],\n",
    "              'max_depth': [None, 10, 20],\n",
    "              'min_samples_split': [2, 5, 10],\n",
    "              'min_samples_leaf': [1, 2, 4],\n",
    "              'max_features': ['sqrt', 'log2', None],\n",
    "              'bootstrap': [True, False],\n",
    "              'criterion': ['gini', 'entropy']}\n",
    "\n",
    "    grid_searchrf = GridSearchCV(rf, param_grid, cv=5, scoring='balanced_accuracy')\n",
    "    grid_searchrf.fit(X_train, y_train) #GirdCV taking a long time\n",
    "    grid_searchrf.score(X_test, y_test)\n",
    "    final_rf = grid_searchrf.best_estimator_\n",
    "    accuracyrf = final_rf.score(X_test, y_test)\n",
    "    print(f\"Model Accuracy on test set ={accuracyrf: .2%}\")\n",
    "    y_pred = final_rf.predict(X_test)\n",
    "\n",
    "    with open('best_rf_model.pkl', 'wb') as file:\n",
    "        pickle.dump(grid_searchrf, file)\n",
    "\n",
    "    accuracyr = accuracy_score(y_test, y_pred)\n",
    "    precisionr = precision_score(y_test, y_pred)\n",
    "    recallr = recall_score(y_test, y_pred)\n",
    "    balance = balanced_accuracy_score(y_test,y_pred)\n",
    "\n",
    "    print(\"Accuracy:\", accuracyr)\n",
    "    print(\"Precision:\", precisionr)#precision and recall not working\n",
    "    print(\"Recall:\", recallr)\n",
    "    print(\"Balanced:\", balance)\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "X_train, X_test, y_train, y_test = data_processing(dataset)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP(X_train, X_test, y_train, y_test)\n",
    "logistic_regression(X_train, X_test, y_train, y_test)\n",
    "RandomForest(X_train, X_test, y_train, y_test)\n",
    "decision_Tree(X_train, X_test, y_train, y_test)\n",
    "svc(X_train, X_test, y_train, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
